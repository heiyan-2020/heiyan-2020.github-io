<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>CS188-P5 Reinforcement Learning | 黑岩</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CS188-P5 Reinforcement Learning</h1><a id="logo" href="/.">黑岩</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CS188-P5 Reinforcement Learning</h1><div class="post-meta">2022-09-05<span> | </span><span class="category"><a href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Motivation"><span class="toc-number">1.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-based-Learning"><span class="toc-number">2.</span> <span class="toc-text">Model-based Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-free-Learning"><span class="toc-number">3.</span> <span class="toc-text">Model-free Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Direct-Evaluation"><span class="toc-number">3.1.</span> <span class="toc-text">Direct Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Difference-Learning"><span class="toc-number">3.2.</span> <span class="toc-text">Temporal Difference Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning"><span class="toc-number">3.3.</span> <span class="toc-text">Q-Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Approximate-Q-Learning"><span class="toc-number">3.4.</span> <span class="toc-text">Approximate Q-Learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Exploration-and-Exploitation"><span class="toc-number">4.</span> <span class="toc-text">Exploration and Exploitation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#epsilon-Greedy"><span class="toc-number">4.1.</span> <span class="toc-text">$\epsilon$-Greedy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exploration-Functions"><span class="toc-number">4.2.</span> <span class="toc-text">Exploration Functions</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#CS188-Project3"><span class="toc-number">5.</span> <span class="toc-text">CS188 Project3</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Q1-Value-Iteration"><span class="toc-number">5.1.</span> <span class="toc-text">Q1 Value Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q2-Bridge-Crossing-Analysis"><span class="toc-number">5.2.</span> <span class="toc-text">Q2 Bridge Crossing Analysis</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q3-Policies"><span class="toc-number">5.3.</span> <span class="toc-text">Q3 Policies</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q4-Asynchronous-Value-Iteration"><span class="toc-number">5.4.</span> <span class="toc-text">Q4 Asynchronous Value Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q5-Prioritized-Sweeping-Value-Iteration"><span class="toc-number">5.5.</span> <span class="toc-text">Q5 Prioritized Sweeping Value Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q6-Q-Learning"><span class="toc-number">5.6.</span> <span class="toc-text">Q6 Q-Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q7-Epsilon-Greedy"><span class="toc-number">5.7.</span> <span class="toc-text">Q7 Epsilon Greedy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q10-Approximate-Q-Learning"><span class="toc-number">5.8.</span> <span class="toc-text">Q10 Approximate Q-Learning</span></a></li></ol></li></ol></div></div><div class="post-content"><p>强化学习用于解决怎样的问题 | 基本的强化学习算法 | CS188 Project 3</p>
<span id="more"></span>

<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>在<a href="https://heiyan-2020.github.io/2022/08/23/CS188-P4-Markov-decision-process/">P4</a>中，我们讨论了如何利用Value Iteration等算法解决MDP问题，这些算法的基本特征是其事先拥有计算optimal policy所需的全部信息。（参考如下贝尔曼方程，所需的全部信息无非$T,R$）<br>$$<br>V^{\star}(s)&#x3D;\underset{a}{max}\underset{s’}{\sum}\space T(s,a,s’)[R(s,a,s’)+\gamma V^{\star}(s’)]<br>$$<br>如果$T,R$未知，我们就需要依靠Learning来获取需要的信息。Learning有很多种，最基础的是监督学习和强化学习。</p>
<ul>
<li><p>监督学习：事先有一些标注好的数据，学习的过程就是训练Agent在遇到类似情况时按照标注数据行动。</p>
<p>监督学习有两大难点：一是标注数据很难覆盖全部情况，比如在棋类游戏中，很难有一个大师数据库能提供所有状态下正确的走法；二是很多应用场景没有标注数据。在这种场景中就需要用到强化学习</p>
</li>
<li><p>强化学习：Agent不断与环境交互并获得反馈，利用这些反馈来修正自己的行为。<strong>反馈</strong>其实也属于先验知识，但它往往比较清晰简单，比如下棋时的胜负，赛车时是否沿赛道行进等。这是强化学习与监督学习本质不同的地方，强化学习所需要的先验知识很简单就能获得，无需标注数据。</p>
</li>
</ul>
<p>根据要学习的目标不同，强化学习可分为两种，以MDP为例：</p>
<ol>
<li>Model-based Learning: 通过Learning获得$T,R$，然后再求optimal policy</li>
<li>Model-free Learning: 直接学习values&#x2F;q-values</li>
</ol>
<p>根据Agent的行为是否改变，强化学习又可分为两种：</p>
<ol>
<li>passive: 事先给定一个policy $\pi$, Agent完全按照$\pi$与环境交互并收集反馈完成学习。<ul>
<li>如果是model-free, 那学习的目标就是$V^{\pi}(s)$, 否则将是Transition model.</li>
<li>因为固定$\pi$显然没办法获取MDP的全部信息，因此passive reinforcement必须和其它方法结合起来使用才能求出真正的optimal policy</li>
</ul>
</li>
<li>active: Agent一边学习一边修正自己的行为，直到学出optimal policy.<ul>
<li>Agent必须尽可能多地与环境产生不同交互才能习得全部知识，这个阶段称为<strong>exploration</strong>. 与此同时，Agent也需要将自己的行为逐渐收敛到optimal policy，这个阶段称为<strong>exploitation</strong>. 如何平衡分配在两个阶段花费的时间是一个<a href="#exploration">值得讨论的话题</a>。</li>
</ul>
</li>
</ol>
<h1 id="Model-based-Learning"><a href="#Model-based-Learning" class="headerlink" title="Model-based Learning"></a>Model-based Learning</h1><p>Model-based Learning算法直接估计$T,R$的估计值$\hat{T},\hat{R}$.</p>
<p><strong>计算$\hat{T}$</strong></p>
<p>选定一个随机的policy $\pi_{explore}$记录从$(s,a)$到$s’$的次数$Cnt[(s,a,s’)]$, 以及到达q-state$(s,a)$的总次数$Cnt[(s,a)]$. 则有：</p>
<p>$\hat{T}(s,a,s’)&#x3D;Cnt[(s,a,s’)]&#x2F;Cnt[(s,a)]&#x3D;f(s’|a,s)$. 其中$f$表示事件的频率</p>
<p>事实上，MDP具有如下性质：$T(s,a,s’)&#x3D;P(s’|a,s)$. 根据<strong>大数定律</strong>，当取样次数足够多时，$\hat{T}$将收敛到$T$</p>
<p>之后尝试不同的policy求出所有$T$</p>
<p><strong>计算$\hat{R}$</strong></p>
<p>从q-state$(s,a)$转移到$s’$时，$R(s,a,s’)$就获知了。因此只要尝试不同的policy直到所有$R$已知即可。</p>
<hr>
<p>在求出$T,R$后，应用P4中讨论过的Policy Iteration即可。</p>
<p>这种方法的弊端是我们需要维护每个tuple的$Cnt$，memory overhead很大。</p>
<h1 id="Model-free-Learning"><a href="#Model-free-Learning" class="headerlink" title="Model-free Learning"></a>Model-free Learning</h1><ol>
<li>passive reinforcement learning<ul>
<li>给定policy $\pi$, 学习出$V^{\pi}(s)$.</li>
</ul>
</li>
<li>active reinforcement learning</li>
</ol>
<h2 id="Direct-Evaluation"><a href="#Direct-Evaluation" class="headerlink" title="Direct Evaluation"></a>Direct Evaluation</h2><p>DE是一种passive reinforcement learning：确定一个随机的policy $\pi$,然后不断采样，统计每个状态$s$获得的总收益$totalValue$, 以及所有样本中到达状态$s$的次数$Cnt$. 采样足够多次数后，$V^{\pi}(s)&#x3D;totalValue(s)&#x2F;Cnt(s)$.</p>
<p>这个想法相当直观，因为$\pi$已经确定，那么从状态$s$出发到终止状态$s_t$所经过的每种状态迁移序列的出现概率 × 该序列收益即为$V^{\pi}(s)$. </p>
<p>采样次数足够多时，$totalValue(s)$中各个tuple$(s,a,s’)$出现的频率收敛到概率，因此等式成立。</p>
<p>这种方法的弊端是需要采样足够多次数，否则结果与真实值相差较大。</p>
<h2 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal Difference Learning"></a>Temporal Difference Learning</h2><p>TD-Learning也是一种passive reinforcement learning，它的核心思想是<strong>从所有经验中学习</strong>。要理解这个思想，首先回顾DE的做法————对于每个样本$(s,a,s’)$，DE仅仅记录其产生的收益和次数，并没有完全利用这里面的状态转移信息。</p>
<p>我们将passive reinforcement learning的目标重新表述为：在不求$T$的情况下直接解下列方程组，更一般的来说就是在不知道权重的情况下求加权平均。TD-Learning通过<strong>指数移动平均法</strong>达成上述目标。<br>$$<br>V^{\pi}(s)&#x3D;\sum_{s’}T(s,\pi(s),s’)[R(s,\pi(s),s’)+\gamma V^{\pi}(s’)]<br>$$</p>
<ul>
<li><p>初始化$V^{\pi}(s)&#x3D;0$</p>
</li>
<li><p>对于每一次状态转移$(s,\pi(s),s’)$, 可以得到值$sample&#x3D;R(s,\pi(s),s’)+\gamma V^{\pi}(s’)$. 该值实际上是对$V^{\pi}(s)$的新估计值。接下来就要把新估计值以某种方式加入到之前的估计值中（反馈）。具体地，我们采用如下更新策略：<br>$$<br>V^{\pi}(s)\leftarrow(1-\alpha)V^{\pi}(s)+\alpha* sample<br>$$<br>其中, $0\leq \alpha \leq 1$. 其含义为学习速率，表示我们对新估计值的认可程度。很显然，当$V^{\pi}$贴近真实值时, $\alpha$越小，收敛速率越快反之亦然。一般来说，整个学习过程中$V^{\pi}$是越来越贴近真值的。因此通常在一开始指定$\alpha&#x3D;1$, 然后逐渐减小$\alpha$的值。</p>
<p>从另一个方面我们也能看出这个更新公式有效的原因，即使假设$\alpha$不变，展开递推式易得:<br>$$<br>V^{\pi}<em>k(s)&#x3D;\alpha\cdot[(1-\alpha)^{k-1}\cdot sample_1+…+(1-\alpha)\cdot sample</em>{k-1}+sample_k]<br>$$<br>因为$0\leq1-\alpha\leq 1$, 因此越老旧的样本在后期比重越小。这很符合直观，因为随着算法的不断迭代，样本的准确值是逐渐上升的，我们当然希望准确的样本能占比更多。</p>
</li>
<li><p>不断迭代直到收敛</p>
</li>
</ul>
<p>再次理解<strong>从所有经验中学习</strong>：在TD-L中，每一个样本包含了状态转移的信息且都在最终的结果有一席之地。</p>
<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>DE和TD-L从本质上来说都只是在求$V^{\pi}$, 而要想真正得到optimal policy还需要借助其它手段，比如model-based learning等。而求出$Q^{*}$则可直接导出optimal policy.</p>
<p>首先，如果$T,R$已知，我们可通过q-value iteration求解，其迭代式如下：<br>$$<br>Q_{k+1}(s,a)&#x3D;\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)]<br>$$<br>然而，权重未知。但这和TD-L形式完全一致：在不知权重的情况下求加权平均。因此继续采用指数移动平均法即可求解。<br>$$<br>sample&#x3D;R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)<br>$$</p>
<p>$$<br>Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha*sample<br>$$</p>
<h2 id="Approximate-Q-Learning"><a href="#Approximate-Q-Learning" class="headerlink" title="Approximate Q-Learning"></a>Approximate Q-Learning</h2><p>Q-L已经足够好了，美中不足的是为了执行该算法需要维护所有的q-value. 一旦状态数太多(而这十分常见)，很容易就会超过计算机内存限制。而很多状态其实是“<strong>相似</strong>”的，没必要逐个学习其q-value而是可以举一反三。</p>
<p><img src="/2022/09/05/CS188-P5-Reinforcement-Learning/image-20220906103334594.png" alt="图1"></p>
<p>以上图为例，Figure1,2,3分别代表三种q-state。在传统Q-L中需要分别维护其q-value然后逐渐迭代收敛。但实际上只要习得Figure1对应的q-value，发现这种状态收益差，就可以直接推断出Figure2、Figure3收益也差，而无需再逐次收敛求解。</p>
<p>这种idea本质上就是<strong>只学习具有代表性的情况</strong>。换句话说，我们需要将原来state的本质特征抽象表示为特征向量，进而可得出:<br>$$<br>V(s)&#x3D;w_1\cdot f_1(s)+w_2\cdot f_2(s)+…+w_n\cdot f_n(s)&#x3D;\vec{w}\cdot\vec{f}(s)<br>$$</p>
<p>$$<br>Q(s,a)&#x3D;\vec{w}\cdot \vec{f}(s,a)<br>$$</p>
<p>其中$f_i$表示特征函数，比如在Pacman中可以是距离最近gohst的距离</p>
<p>对$V(s)$的重构使得我们的问题变成了学习参数$\vec{w}$，操作如下：</p>
<p>定义$diff&#x3D;[R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)]-Q(s,a)$, 则更新规则如下:<br>$$<br>w_i\leftarrow w_i+\alpha\cdot diff\cdot f_i(s,a)<br>$$<br>如此，我们就只需要在内存中维护$\vec{w}$, 在学习结束后按需计算$Q^{*}$即可。</p>
<blockquote>
<p>Q: AQ-L的idea和最终的算法是怎么联系到一起的？</p>
</blockquote>
<p>在上文中我们看到AQ-L的启发式动机是，在Q-L中学习了大量重复的内容。因此为了优化，我们希望的是对于相似的情况只学习一遍，其它情况由这一遍习得的结果推断得出。这个idea促使我们不再学习具体的state’s value，而是学习从相似states中抽象而来的特征的value. 而一旦抽象出特征，value就可立马表示成线性值函数，需要学习的内容进一步变成了值函数的参数。</p>
<h1 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h1><p><a name="exploration" alt="none"> </a></p>
<h2 id="epsilon-Greedy"><a href="#epsilon-Greedy" class="headerlink" title="$\epsilon$-Greedy"></a>$\epsilon$-Greedy</h2><p>想法非常简单，选定$\epsilon\in[0,1]$。</p>
<p>在每一次选择action时以概率$\epsilon$采用random policy（即在所有legalActions中随机选择）, 以概率$1-\epsilon$采用当前的estimated optimal action. </p>
<p>在实际应用中$\epsilon$需要人为tune. Project3中有类似的题目，见下文。</p>
<h2 id="Exploration-Functions"><a href="#Exploration-Functions" class="headerlink" title="Exploration Functions"></a>Exploration Functions</h2><p>更新规则变为:<br>$$<br>Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha\cdot [R(s,a,s’)+\gamma \underset{a’}{max}f(s’,a’)]<br>$$<br>$f$有很多种定义，较为常见的是如下:<br>$$<br>f(s,a)&#x3D;Q(s,a)+\frac{k}{N(s,a)}<br>$$<br>其中$k$是一个提前确定的常数,$N(s,a)$表示该q-state被访问的次数</p>
<blockquote>
<p>Q: Exploitation和Exploration分别有什么意义？</p>
</blockquote>
<p><strong>指数平均移动法</strong>收敛到一个q-value的真值需要多轮采样。学习完毕后Agent的行为是利用学到的q-values计算optimal policy，因此学习阶段的目标是让真正optimal的q-state“凸显”出来。如果没有exploitation阶段，那最优策略的收益离真值较远，有可能被次优解给挤掉了。形象一点来说，exploitation阶段的目标是发掘当前最优策略的真正价值，避免被埋没。</p>
<p>当然，也不能只有exploitation，这同样会使得最优解不一定被发现。</p>
<h1 id="CS188-Project3"><a href="#CS188-Project3" class="headerlink" title="CS188 Project3"></a>CS188 Project3</h1><h2 id="Q1-Value-Iteration"><a href="#Q1-Value-Iteration" class="headerlink" title="Q1 Value Iteration"></a>Q1 Value Iteration</h2><p>将贝尔曼方程原模原样搬上来即可。稍微值得注意的是，每一轮迭代更新$s$时依赖的$state$可能已经被更新了，因此需要先保存所有$states$上一轮迭代的值用作更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">runValueIteration</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.iterations):</span><br><span class="line">        incrementVector = util.Counter()</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> self.mdp.getStates():</span><br><span class="line">            maxValue = -<span class="number">9999</span></span><br><span class="line">            actions = self.mdp.getPossibleActions(s)</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(actions) == <span class="number">0</span>:</span><br><span class="line">                maxValue = <span class="number">0</span></span><br><span class="line">        	<span class="keyword">for</span> a <span class="keyword">in</span> actions:</span><br><span class="line">                    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">                    <span class="keyword">for</span> sp_tuple <span class="keyword">in</span> self.mdp.getTransitionStatesAndProbs(s, a):</span><br><span class="line">                        succ, prob = sp_tuple</span><br><span class="line">                        reward = self.mdp.getReward(s, a, succ)</span><br><span class="line">                        <span class="built_in">sum</span> += prob * (reward + self.discount * self.values[succ])</span><br><span class="line">                    maxValue = <span class="built_in">max</span>(<span class="built_in">sum</span>, maxValue)</span><br><span class="line">                incrementVector[s] = maxValue - self.values[s]</span><br><span class="line">            self.values = self.values + incrementVector</span><br><span class="line">            </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeQValueFromValues</span>(<span class="params">self, state, action</span>):</span><br><span class="line">    <span class="built_in">sum</span> = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> sp_tuple <span class="keyword">in</span> self.mdp.getTransitionStatesAndProbs(state, action):</span><br><span class="line">        succ, prob = sp_tuple</span><br><span class="line">        reward = self.mdp.getReward(state, action, succ)</span><br><span class="line">        <span class="built_in">sum</span> += prob * (reward + self.discount * self.values[succ])</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">computeActionFromValues</span>(<span class="params">self, state</span>):</span><br><span class="line">    cnt = util.Counter()</span><br><span class="line">    actions = self.mdp.getPossibleActions(state)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(actions) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> action <span class="keyword">in</span> actions:</span><br><span class="line">        <span class="keyword">for</span> sp_tuple <span class="keyword">in</span> self.mdp.getTransitionStatesAndProbs(state, action):</span><br><span class="line">            succ, prob = sp_tuple</span><br><span class="line">            reward = self.mdp.getReward(state, action, succ)</span><br><span class="line">            cnt[action] += prob * (reward + self.discount * self.values[succ])</span><br><span class="line">    <span class="keyword">return</span> cnt.argMax()</span><br></pre></td></tr></table></figure>

<h2 id="Q2-Bridge-Crossing-Analysis"><a href="#Q2-Bridge-Crossing-Analysis" class="headerlink" title="Q2 Bridge Crossing Analysis"></a>Q2 Bridge Crossing Analysis</h2><p>这题很有意思，让你自己调整MDP的参数使得Agent的行为符合预期规定。通过这题可以很好地体验各个参数的作用。<br>$$<br>discount&#x3D;0.9,noise&#x3D;0<br>$$</p>
<h2 id="Q3-Policies"><a href="#Q3-Policies" class="headerlink" title="Q3 Policies"></a>Q3 Policies</h2><p>Q3和上题考察目标一样，不过地图变得更加复杂了。</p>
<img src="CS188-P5-Reinforcement-Learning/image-20220908170521798.png" alt="image-20220908170521798" style="zoom:33%;" />

<table>
<thead>
<tr>
<th>目标</th>
<th>discount</th>
<th>noise</th>
<th>living reward</th>
<th>解释</th>
</tr>
</thead>
<tbody><tr>
<td>冒险去+1</td>
<td>0.01</td>
<td>0</td>
<td>0.01</td>
<td>尽快结束(高衰变率)，不易落入陷阱(低随机性)</td>
</tr>
<tr>
<td>避险去+1</td>
<td>0.2</td>
<td>0.2</td>
<td>0.01</td>
<td>尽快结束，易落入陷阱</td>
</tr>
<tr>
<td>冒险去+10</td>
<td>0.9</td>
<td>0</td>
<td>0.01</td>
<td>不用尽快结束，不易落入陷阱</td>
</tr>
<tr>
<td>避免去+10</td>
<td>0.9</td>
<td>0.5</td>
<td>0.01</td>
<td>同理</td>
</tr>
<tr>
<td>既不想结束也不想落入陷阱</td>
<td>0.1</td>
<td>0.2</td>
<td>0.01</td>
<td>活着的收益比结束带来的收益大，略微减少discount即可</td>
</tr>
</tbody></table>
<h2 id="Q4-Asynchronous-Value-Iteration"><a href="#Q4-Asynchronous-Value-Iteration" class="headerlink" title="Q4 Asynchronous Value Iteration"></a>Q4 Asynchronous Value Iteration</h2><p>和Q1基本相同，唯一区别在于这回一次迭代只更新一个$state$, 因此依赖的其它$state$都是最新值。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">runValueIteration</span>(<span class="params">self</span>):</span><br><span class="line">    states = self.mdp.getStates()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.iterations):</span><br><span class="line">        state = states[i % <span class="built_in">len</span>(states)]</span><br><span class="line">        self.values[state] = computeMaxQvalue(state, self.mdp, self.discount,self.values)</span><br></pre></td></tr></table></figure>

<h2 id="Q5-Prioritized-Sweeping-Value-Iteration"><a href="#Q5-Prioritized-Sweeping-Value-Iteration" class="headerlink" title="Q5 Prioritized Sweeping Value Iteration"></a>Q5 Prioritized Sweeping Value Iteration</h2><p>用优先级队列优化迭代。</p>
<p>ValueIteration的目标是尽可能早地迭代至不动点，此时就一定是最优解。引入优先级队列就是要让最有可能造成policy改变的state优先迭代。</p>
<ul>
<li><p>怎样的state最可能改变自己当前的optimal action？</p>
<p>$$ V(s)&#x3D;\underset{a}{max}[Q(s,a)] $$</p>
<p>根据上述公式，即问最大$Q(s,a)$中的a什么时候最可能发生改变？</p>
<p>答案是：如果在迭代后继节点时造成$maxQ$与当前的$V$差值较大，此时更可能改变。但并不是一定，因为有可能是相同action对应的Q发生大改。但这种启发式的衡量方式计算简便且准确度并不低。</p>
</li>
<li><p>为什么要引入临界值$\theta$？</p>
<p>当差值较小时，我们不希望重新把前驱结点加入更新队列，$\theta$用于衡量这一临界值。换句话说，$\theta$越大，迭代次数就越少，迭代结果就越不准确。</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">runValueIteration</span>(<span class="params">self</span>):</span><br><span class="line">    states = self.mdp.getStates()</span><br><span class="line">    <span class="comment">#compute all predecessors.</span></span><br><span class="line">    predecessors = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states:</span><br><span class="line">        successors = <span class="built_in">set</span>()</span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> self.mdp.getPossibleActions(state):</span><br><span class="line">            <span class="keyword">for</span> succ <span class="keyword">in</span> self.mdp.getTransitionStatesAndProbs(state, action)[<span class="number">0</span>]:</span><br><span class="line">                successors.add(succ)</span><br><span class="line">        <span class="keyword">for</span> succ <span class="keyword">in</span> successors:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> succ <span class="keyword">in</span> predecessors:</span><br><span class="line">                predecessors[succ] = <span class="built_in">set</span>()</span><br><span class="line">            predecessors[succ].add(state)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#initialize priority queue.</span></span><br><span class="line">    priority_queue = util.PriorityQueue()</span><br><span class="line">    <span class="keyword">for</span> state <span class="keyword">in</span> states:</span><br><span class="line">        diff = <span class="built_in">abs</span>(computeMaxQvalue(state, self.mdp, self.discount, self.values) - self.values[state])</span><br><span class="line">        priority_queue.push(state, -diff)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#iteration</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, self.iterations):</span><br><span class="line">        <span class="keyword">if</span> priority_queue.isEmpty():</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        state = priority_queue.pop()</span><br><span class="line">        self.values[state] = computeMaxQvalue(state, self.mdp, self.discount, self.values)</span><br><span class="line">        <span class="keyword">for</span> pred <span class="keyword">in</span> predecessors[state]:</span><br><span class="line">            diff = <span class="built_in">abs</span>(computeMaxQvalue(pred, self.mdp, self.discount, self.values) - self.values[pred])</span><br><span class="line">            <span class="keyword">if</span> diff &gt; self.theta:</span><br><span class="line">                priority_queue.update(pred, -diff)</span><br></pre></td></tr></table></figure>

<h2 id="Q6-Q-Learning"><a href="#Q6-Q-Learning" class="headerlink" title="Q6 Q-Learning"></a>Q6 Q-Learning</h2><p>当你理解了Q-Learning为什么work后，这一节无非就是把公式转换为代码罢了。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, state, action, nextState, reward</span>):</span><br><span class="line">    sample = reward + self.discount * self.computeValueFromQValues(nextState)</span><br><span class="line">    self.qvalues[(state, action)] = self.getQValue(state, action) * (<span class="number">1</span> - self.alpha) + sample * self.alpha</span><br></pre></td></tr></table></figure>

<h2 id="Q7-Epsilon-Greedy"><a href="#Q7-Epsilon-Greedy" class="headerlink" title="Q7 Epsilon Greedy"></a>Q7 Epsilon Greedy</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getAction</span>(<span class="params">self, state</span>):</span><br><span class="line">    <span class="keyword">if</span> util.flipCoin(self.epsilon):</span><br><span class="line">      action = random.choice(legalActions)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      action = self.computeActionFromQValues(state)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>

<p>Q8，Q9都不用修改代码，只要前面的代码足够general即可</p>
<h2 id="Q10-Approximate-Q-Learning"><a href="#Q10-Approximate-Q-Learning" class="headerlink" title="Q10 Approximate Q-Learning"></a>Q10 Approximate Q-Learning</h2><p>同上，只要理解了公式，代码无非就是表达公式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getQValue</span>(<span class="params">self, state, action</span>):</span><br><span class="line">    qvalue = <span class="number">0</span></span><br><span class="line">    features = self.featExtractor.getFeatures(state, action)</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> <span class="built_in">list</span>(features.keys()):</span><br><span class="line">      qvalue += self.weights[feat] * features[feat]</span><br><span class="line">    <span class="keyword">return</span> qvalue</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">update</span>(<span class="params">self, state, action, nextState, reward</span>):</span><br><span class="line">    diff = reward + self.discount * self.computeValueFromQValues(nextState) - self.getQValue(state, action)</span><br><span class="line">    features = self.featExtractor.getFeatures(state, action)</span><br><span class="line">    <span class="keyword">for</span> feat <span class="keyword">in</span> <span class="built_in">list</span>(features.keys()):</span><br><span class="line">      self.weights[feat] += self.alpha * diff * features[feat]</span><br></pre></td></tr></table></figure>

<p>这道题在正确性之外有一点值得注意：</p>
<p>经典Q-Learning在Pacman地图size很小时表现尚可，可一旦地图扩大，学2000轮都还是在乱走；而使用近似Q-Learning后，学50轮就表现得非常不错了，为什么？</p>
<ul>
<li>地图size扩大后，状态数激增（经典Q-Learning中地图上任意一点不一样都是不同state），因此学习阶段能覆盖的policy占比很少。换句话说，即使学习时间全部用于exploration，都可能还没覆盖到optimal policy。</li>
<li>近似Q-Learning抽取原来状态的重要特征，<strong>泛化</strong>了学习。因此能够在尽可能少的学习时间里，找到最优解。</li>
</ul>
<p>这个现象说明近似Q-Learning不光节省空间，只要特征抽取合理也能极大地提高学习效率。</p>
</div><div class="tags"><a href="/tags/人工智能"><i class="fa fa-tag">人工智能</i></a></div><div class="post-nav"><a class="pre" href="/2022/09/21/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%BD%9C%E4%B8%9A/">云计算作业</a><a class="next" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://heiyan-2020.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img src="/img/avatar.jpg"/></a><p>态度是最重要的默认参数。</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15px;">人工智能</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E6%83%B3/" style="font-size: 15px;">生活随想</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/21/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%BD%9C%E4%B8%9A/">云计算作业</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/09/05/CS188-P5-Reinforcement-Learning/">CS188-P5 Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/21/CS188-P3-Game/">CS188-P3 Game</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/20/CS188-P2-CSP/">CS188-P2 CSP</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/18/CS188-P1-Searching/">CS188-P1 Searching</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/15/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%9F/">什么是计算思维？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/22/%E6%88%91%E7%9A%84%E6%96%B0%E5%A4%A9%E5%9C%B0/">我的新天地</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://icstorm.top/" title="永远の咣队" target="_blank">永远の咣队</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">黑岩.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>