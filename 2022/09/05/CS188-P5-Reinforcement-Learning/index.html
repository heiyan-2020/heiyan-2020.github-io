<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>CS188-P5 Reinforcement Learning | 黑岩</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CS188-P5 Reinforcement Learning</h1><a id="logo" href="/.">黑岩</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CS188-P5 Reinforcement Learning</h1><div class="post-meta">2022-09-05<span> | </span><span class="category"><a href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Motivation"><span class="toc-number">1.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-based-Learning"><span class="toc-number">2.</span> <span class="toc-text">Model-based Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-free-Learning"><span class="toc-number">3.</span> <span class="toc-text">Model-free Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Direct-Evaluation"><span class="toc-number">3.1.</span> <span class="toc-text">Direct Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Difference-Learning"><span class="toc-number">3.2.</span> <span class="toc-text">Temporal Difference Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning"><span class="toc-number">3.3.</span> <span class="toc-text">Q-Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Approximate-Q-Learning"><span class="toc-number">3.4.</span> <span class="toc-text">Approximate Q-Learning</span></a></li></ol></li></ol></div></div><div class="post-content"><p>强化学习初探</p>
<span id="more"></span>

<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>在<a href="https://heiyan-2020.github.io/2022/08/23/CS188-P4-Markov-decision-process/">P4</a>中，我们讨论了如何利用Value Iteration等算法解决MDP问题，这些算法的基本特征是其事先拥有计算optimal policy所需的全部信息。（参考如下贝尔曼方程，所需的全部信息无非$T,R$）<br>$$<br>V^{\star}(s)&#x3D;\underset{a}{max}\underset{s’}{\sum}\space T(s,a,s’)[R(s,a,s’)+\gamma V^{\star}(s’)]<br>$$<br>如果$T,R$未知，我们就需要依靠Learning来获取需要的信息。强化学习的基本思想是让Agent在该问题上不断探索————采取一系列行为(采样)，获得相应的反馈，然后用这些反馈来估计要学习的值。根据要学习的值不同，强化学习又可分为两种：</p>
<ol>
<li>Model-based Learning: 通过Learning获得$T,R$，然后再求optimal policy</li>
<li>Model-free Learning: 直接学习values&#x2F;q-values</li>
</ol>
<p><em>[补充] MDP如何采样？</em></p>
<ul>
<li>确定一个策略$\pi$。从某个起始状态开始按照$\pi$行动直到终止状态，从起始到终止这一整个过程称为一个episode.</li>
</ul>
<h1 id="Model-based-Learning"><a href="#Model-based-Learning" class="headerlink" title="Model-based Learning"></a>Model-based Learning</h1><p>Model-based Learning算法直接估计$T,R$的估计值$\hat{T},\hat{R}$.</p>
<p><strong>计算$\hat{T}$</strong></p>
<p>选定一个随机的policy $\pi_{explore}$记录从$(s,a)$到$s’$的次数$Cnt[(s,a,s’)]$, 以及到达q-state$(s,a)$的总次数$Cnt[(s,a)]$. 则有：</p>
<p>$\hat{T}(s,a,s’)&#x3D;Cnt[(s,a,s’)]&#x2F;Cnt[(s,a)]&#x3D;f(s’|a,s)$. 其中$f$表示事件的频率</p>
<p>事实上，MDP具有如下性质：$T(s,a,s’)&#x3D;P(s’|a,s)$. 根据<strong>大数定律</strong>，当取样次数足够多时，$\hat{T}$将收敛到$T$</p>
<p>之后尝试不同的policy求出所有$T$</p>
<p><strong>计算$\hat{R}$</strong></p>
<p>从q-state$(s,a)$转移到$s’$时，$R(s,a,s’)$就获知了。因此只要尝试不同的policy直到所有$R$已知即可。</p>
<hr>
<p>在求出$T,R$后，应用P4中讨论过的Policy Iteration即可。</p>
<p>这种方法的弊端是我们需要维护每个tuple的$Cnt$，memory overhead很大。</p>
<h1 id="Model-free-Learning"><a href="#Model-free-Learning" class="headerlink" title="Model-free Learning"></a>Model-free Learning</h1><p>Model-free Learning可根据[…]分为两种：</p>
<ol>
<li>passive reinforcement learning<ul>
<li>给定policy $\pi$, 学习出$V^{\pi}(s)$.</li>
</ul>
</li>
<li>active reinforcement learning</li>
</ol>
<h2 id="Direct-Evaluation"><a href="#Direct-Evaluation" class="headerlink" title="Direct Evaluation"></a>Direct Evaluation</h2><p>DE是一种passive reinforcement learning：确定一个随机的policy $\pi$,然后不断采样，统计每个状态$s$获得的总收益$totalValue$, 以及所有样本中到达状态$s$的次数$Cnt$. 采样足够多次数后，$V^{\pi}(s)&#x3D;totalValue(s)&#x2F;Cnt(s)$.</p>
<p>这个想法相当直观，因为$\pi$已经确定，那么从状态$s$出发到终止状态$s_t$所经过的每种状态迁移序列的出现概率 × 该序列收益即为$V^{\pi}(s)$. </p>
<p>采样次数足够多时，$totalValue(s)$中各个tuple$(s,a,s’)$出现的频率收敛到概率，因此等式成立。</p>
<p>这种方法的弊端是需要采样足够多次数，否则结果与真实值相差较大。</p>
<h2 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal Difference Learning"></a>Temporal Difference Learning</h2><p>TD-Learning也是一种passive reinforcement learning，它的核心思想是<strong>从所有经验中学习</strong>。要理解这个思想，首先回顾DE的做法————对于每个样本$(s,a,s’)$，DE仅仅记录其产生的收益和次数，并没有完全利用这里面的状态转移信息。</p>
<p>我们将passive reinforcement learning的目标重新表述为：在不求$T$的情况下直接解下列方程组，更一般的来说就是在不知道权重的情况下求加权平均。TD-Learning通过<strong>指数移动平均法</strong>达成上述目标。<br>$$<br>V^{\pi}(s)&#x3D;\sum_{s’}T(s,\pi(s),s’)[R(s,\pi(s),s’)+\gamma V^{\pi}(s’)]<br>$$</p>
<ul>
<li><p>初始化$V^{\pi}(s)&#x3D;0$</p>
</li>
<li><p>对于每一次状态转移$(s,\pi(s),s’)$, 可以得到值$sample&#x3D;R(s,\pi(s),s’)+\gamma V^{\pi}(s’)$. 该值实际上是对$V^{\pi}(s)$的新估计值。接下来就要把新估计值以某种方式加入到之前的估计值中（反馈）。具体地，我们采用如下更新策略：<br>$$<br>V^{\pi}(s)\leftarrow(1-\alpha)V^{\pi}(s)+\alpha* sample<br>$$<br>其中, $0\leq \alpha \leq 1$. 其含义为学习速率，表示我们对新估计值的认可程度。很显然，当$V^{\pi}$贴近真实值时, $\alpha$越小，收敛速率越快反之亦然。一般来说，整个学习过程中$V^{\pi}$是越来越贴近真值的。因此通常在一开始指定$\alpha&#x3D;1$, 然后逐渐减小$\alpha$的值。</p>
<p>从另一个方面我们也能看出这个更新公式有效的原因，即使假设$\alpha$不变，展开递推式易得:<br>$$<br>V^{\pi}<em>k(s)&#x3D;\alpha\cdot[(1-\alpha)^{k-1}\cdot sample_1+…+(1-\alpha)\cdot sample</em>{k-1}+sample_k]<br>$$<br>因为$0\leq1-\alpha\leq 1$, 因此越老旧的样本在后期比重越小。这很符合直观，因为随着算法的不断迭代，样本的准确值是逐渐上升的，我们当然希望准确的样本能占比更多。</p>
</li>
<li><p>不断迭代直到收敛</p>
</li>
</ul>
<p>再次理解<strong>从所有经验中学习</strong>：在TD-L中，每一个样本包含了状态转移的信息且都在最终的结果有一席之地。</p>
<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>DE和TD-L从本质上来说都只是在求$V^{\pi}$, 而要想真正得到optimal policy还需要借助其它手段，比如model-based learning等。而求出$Q^{*}$则可直接导出optimal policy.</p>
<p>首先，如果$T,R$已知，我们可通过q-value iteration求解，其迭代式如下：<br>$$<br>Q_{k+1}(s,a)&#x3D;\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)]<br>$$<br>然而，权重未知。但这和TD-L形式完全一致：在不知权重的情况下求加权平均。因此继续采用指数移动平均法即可求解。<br>$$<br>sample&#x3D;R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)<br>$$</p>
<p>$$<br>Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha*sample<br>$$</p>
<h2 id="Approximate-Q-Learning"><a href="#Approximate-Q-Learning" class="headerlink" title="Approximate Q-Learning"></a>Approximate Q-Learning</h2><p>Q-L已经足够好了，美中不足的是为了执行该算法需要维护所有的q-value. 一旦状态数太多(而这十分常见)，很容易就会超过计算机内存限制。</p>
<p>TBD</p>
</div><div class="tags"><a href="/tags/人工智能"><i class="fa fa-tag">人工智能</i></a></div><div class="post-nav"><a class="next" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://heiyan-2020.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img src="/img/avatar.jpg"/></a><p>态度是最重要的默认参数。</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15px;">人工智能</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E6%83%B3/" style="font-size: 15px;">生活随想</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/05/CS188-P5-Reinforcement-Learning/">CS188-P5 Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/21/CS188-P3-Game/">CS188-P3 Game</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/20/CS188-P2-CSP/">CS188-P2 CSP</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/18/CS188-P1-Searching/">CS188-P1 Searching</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/15/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%9F/">什么是计算思维？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/22/%E6%88%91%E7%9A%84%E6%96%B0%E5%A4%A9%E5%9C%B0/">我的新天地</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://icstorm.top/" title="永远の咣队" target="_blank">永远の咣队</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">黑岩.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>