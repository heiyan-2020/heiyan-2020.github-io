<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>CS188-P5 Reinforcement Learning | 黑岩</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CS188-P5 Reinforcement Learning</h1><a id="logo" href="/.">黑岩</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CS188-P5 Reinforcement Learning</h1><div class="post-meta">2022-09-05<span> | </span><span class="category"><a href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Motivation"><span class="toc-number">1.</span> <span class="toc-text">Motivation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-based-Learning"><span class="toc-number">2.</span> <span class="toc-text">Model-based Learning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Model-free-Learning"><span class="toc-number">3.</span> <span class="toc-text">Model-free Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Direct-Evaluation"><span class="toc-number">3.1.</span> <span class="toc-text">Direct Evaluation</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Temporal-Difference-Learning"><span class="toc-number">3.2.</span> <span class="toc-text">Temporal Difference Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q-Learning"><span class="toc-number">3.3.</span> <span class="toc-text">Q-Learning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Approximate-Q-Learning"><span class="toc-number">3.4.</span> <span class="toc-text">Approximate Q-Learning</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Exploration-and-Exploitation"><span class="toc-number">4.</span> <span class="toc-text">Exploration and Exploitation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#epsilon-Greedy"><span class="toc-number">4.1.</span> <span class="toc-text">$\epsilon$-Greedy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exploration-Functions"><span class="toc-number">4.2.</span> <span class="toc-text">Exploration Functions</span></a></li></ol></li></ol></div></div><div class="post-content"><p>强化学习初探</p>
<span id="more"></span>

<h1 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h1><p>在<a href="https://heiyan-2020.github.io/2022/08/23/CS188-P4-Markov-decision-process/">P4</a>中，我们讨论了如何利用Value Iteration等算法解决MDP问题，这些算法的基本特征是其事先拥有计算optimal policy所需的全部信息。（参考如下贝尔曼方程，所需的全部信息无非$T,R$）<br>$$<br>V^{\star}(s)&#x3D;\underset{a}{max}\underset{s’}{\sum}\space T(s,a,s’)[R(s,a,s’)+\gamma V^{\star}(s’)]<br>$$<br>如果$T,R$未知，我们就需要依靠Learning来获取需要的信息。Learning有很多种，最基础的是监督学习和强化学习。</p>
<ul>
<li><p>监督学习：事先有一些标注好的数据，学习的过程就是训练Agent在遇到类似情况时按照标注数据行动。</p>
<p>监督学习有两大难点：一是标注数据很难覆盖全部情况，比如在棋类游戏中，很难有一个大师数据库能提供所有状态下正确的走法；二是很多应用场景没有标注数据。在这种场景中就需要用到强化学习</p>
</li>
<li><p>强化学习：Agent不断与环境交互并获得反馈，利用这些反馈来修正自己的行为。<strong>反馈</strong>其实也属于先验知识，但它往往比较清晰简单，比如下棋时的胜负，赛车时是否沿赛道行进等。这是强化学习与监督学习本质不同的地方，强化学习所需要的先验知识很简单就能获得，无需标注数据。</p>
</li>
</ul>
<p>根据要学习的目标不同，强化学习可分为两种，以MDP为例：</p>
<ol>
<li>Model-based Learning: 通过Learning获得$T,R$，然后再求optimal policy</li>
<li>Model-free Learning: 直接学习values&#x2F;q-values</li>
</ol>
<p>根据Agent的行为是否改变，强化学习又可分为两种：</p>
<ol>
<li>passive: 事先给定一个policy $\pi$, Agent完全按照$\pi$与环境交互并收集反馈完成学习。<ul>
<li>如果是model-free, 那学习的目标就是$V^{\pi}(s)$, 否则将是Transition model.</li>
<li>因为固定$\pi$显然没办法获取MDP的全部信息，因此passive reinforcement必须和其它方法结合起来使用才能求出真正的optimal policy</li>
</ul>
</li>
<li>active: Agent一边学习一边修正自己的行为，直到学出optimal policy.<ul>
<li>Agent必须尽可能多地与环境产生不同交互才能习得全部知识，这个阶段称为<strong>exploration</strong>. 与此同时，Agent也需要将自己的行为逐渐收敛到optimal policy，这个阶段称为<strong>exploitation</strong>. 如何平衡分配在两个阶段花费的时间是一个<a href="#exploration">值得讨论的话题</a>。</li>
</ul>
</li>
</ol>
<h1 id="Model-based-Learning"><a href="#Model-based-Learning" class="headerlink" title="Model-based Learning"></a>Model-based Learning</h1><p>Model-based Learning算法直接估计$T,R$的估计值$\hat{T},\hat{R}$.</p>
<p><strong>计算$\hat{T}$</strong></p>
<p>选定一个随机的policy $\pi_{explore}$记录从$(s,a)$到$s’$的次数$Cnt[(s,a,s’)]$, 以及到达q-state$(s,a)$的总次数$Cnt[(s,a)]$. 则有：</p>
<p>$\hat{T}(s,a,s’)&#x3D;Cnt[(s,a,s’)]&#x2F;Cnt[(s,a)]&#x3D;f(s’|a,s)$. 其中$f$表示事件的频率</p>
<p>事实上，MDP具有如下性质：$T(s,a,s’)&#x3D;P(s’|a,s)$. 根据<strong>大数定律</strong>，当取样次数足够多时，$\hat{T}$将收敛到$T$</p>
<p>之后尝试不同的policy求出所有$T$</p>
<p><strong>计算$\hat{R}$</strong></p>
<p>从q-state$(s,a)$转移到$s’$时，$R(s,a,s’)$就获知了。因此只要尝试不同的policy直到所有$R$已知即可。</p>
<hr>
<p>在求出$T,R$后，应用P4中讨论过的Policy Iteration即可。</p>
<p>这种方法的弊端是我们需要维护每个tuple的$Cnt$，memory overhead很大。</p>
<h1 id="Model-free-Learning"><a href="#Model-free-Learning" class="headerlink" title="Model-free Learning"></a>Model-free Learning</h1><ol>
<li>passive reinforcement learning<ul>
<li>给定policy $\pi$, 学习出$V^{\pi}(s)$.</li>
</ul>
</li>
<li>active reinforcement learning</li>
</ol>
<h2 id="Direct-Evaluation"><a href="#Direct-Evaluation" class="headerlink" title="Direct Evaluation"></a>Direct Evaluation</h2><p>DE是一种passive reinforcement learning：确定一个随机的policy $\pi$,然后不断采样，统计每个状态$s$获得的总收益$totalValue$, 以及所有样本中到达状态$s$的次数$Cnt$. 采样足够多次数后，$V^{\pi}(s)&#x3D;totalValue(s)&#x2F;Cnt(s)$.</p>
<p>这个想法相当直观，因为$\pi$已经确定，那么从状态$s$出发到终止状态$s_t$所经过的每种状态迁移序列的出现概率 × 该序列收益即为$V^{\pi}(s)$. </p>
<p>采样次数足够多时，$totalValue(s)$中各个tuple$(s,a,s’)$出现的频率收敛到概率，因此等式成立。</p>
<p>这种方法的弊端是需要采样足够多次数，否则结果与真实值相差较大。</p>
<h2 id="Temporal-Difference-Learning"><a href="#Temporal-Difference-Learning" class="headerlink" title="Temporal Difference Learning"></a>Temporal Difference Learning</h2><p>TD-Learning也是一种passive reinforcement learning，它的核心思想是<strong>从所有经验中学习</strong>。要理解这个思想，首先回顾DE的做法————对于每个样本$(s,a,s’)$，DE仅仅记录其产生的收益和次数，并没有完全利用这里面的状态转移信息。</p>
<p>我们将passive reinforcement learning的目标重新表述为：在不求$T$的情况下直接解下列方程组，更一般的来说就是在不知道权重的情况下求加权平均。TD-Learning通过<strong>指数移动平均法</strong>达成上述目标。<br>$$<br>V^{\pi}(s)&#x3D;\sum_{s’}T(s,\pi(s),s’)[R(s,\pi(s),s’)+\gamma V^{\pi}(s’)]<br>$$</p>
<ul>
<li><p>初始化$V^{\pi}(s)&#x3D;0$</p>
</li>
<li><p>对于每一次状态转移$(s,\pi(s),s’)$, 可以得到值$sample&#x3D;R(s,\pi(s),s’)+\gamma V^{\pi}(s’)$. 该值实际上是对$V^{\pi}(s)$的新估计值。接下来就要把新估计值以某种方式加入到之前的估计值中（反馈）。具体地，我们采用如下更新策略：<br>$$<br>V^{\pi}(s)\leftarrow(1-\alpha)V^{\pi}(s)+\alpha* sample<br>$$<br>其中, $0\leq \alpha \leq 1$. 其含义为学习速率，表示我们对新估计值的认可程度。很显然，当$V^{\pi}$贴近真实值时, $\alpha$越小，收敛速率越快反之亦然。一般来说，整个学习过程中$V^{\pi}$是越来越贴近真值的。因此通常在一开始指定$\alpha&#x3D;1$, 然后逐渐减小$\alpha$的值。</p>
<p>从另一个方面我们也能看出这个更新公式有效的原因，即使假设$\alpha$不变，展开递推式易得:<br>$$<br>V^{\pi}<em>k(s)&#x3D;\alpha\cdot[(1-\alpha)^{k-1}\cdot sample_1+…+(1-\alpha)\cdot sample</em>{k-1}+sample_k]<br>$$<br>因为$0\leq1-\alpha\leq 1$, 因此越老旧的样本在后期比重越小。这很符合直观，因为随着算法的不断迭代，样本的准确值是逐渐上升的，我们当然希望准确的样本能占比更多。</p>
</li>
<li><p>不断迭代直到收敛</p>
</li>
</ul>
<p>再次理解<strong>从所有经验中学习</strong>：在TD-L中，每一个样本包含了状态转移的信息且都在最终的结果有一席之地。</p>
<h2 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h2><p>DE和TD-L从本质上来说都只是在求$V^{\pi}$, 而要想真正得到optimal policy还需要借助其它手段，比如model-based learning等。而求出$Q^{*}$则可直接导出optimal policy.</p>
<p>首先，如果$T,R$已知，我们可通过q-value iteration求解，其迭代式如下：<br>$$<br>Q_{k+1}(s,a)&#x3D;\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)]<br>$$<br>然而，权重未知。但这和TD-L形式完全一致：在不知权重的情况下求加权平均。因此继续采用指数移动平均法即可求解。<br>$$<br>sample&#x3D;R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)<br>$$</p>
<p>$$<br>Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha*sample<br>$$</p>
<h2 id="Approximate-Q-Learning"><a href="#Approximate-Q-Learning" class="headerlink" title="Approximate Q-Learning"></a>Approximate Q-Learning</h2><p>Q-L已经足够好了，美中不足的是为了执行该算法需要维护所有的q-value. 一旦状态数太多(而这十分常见)，很容易就会超过计算机内存限制。而很多状态其实是“<strong>相似</strong>”的，没必要逐个学习其q-value而是可以举一反三。</p>
<p><img src="/2022/09/05/CS188-P5-Reinforcement-Learning/image-20220906103334594.png" alt="图1"></p>
<p>以上图为例，Figure1,2,3分别代表三种q-state。在传统Q-L中需要分别维护其q-value然后逐渐迭代收敛。但实际上只要习得Figure1对应的q-value，发现这种状态收益差，就可以直接推断出Figure2、Figure3收益也差，而无需再逐次收敛求解。</p>
<p>这种idea本质上就是<strong>只学习具有代表性的情况</strong>。换句话说，我们需要将原来state的本质特征抽象表示为特征向量，进而可得出:<br>$$<br>V(s)&#x3D;w_1\cdot f_1(s)+w_2\cdot f_2(s)+…+w_n\cdot f_n(s)&#x3D;\vec{w}\cdot\vec{f}(s)<br>$$</p>
<p>$$<br>Q(s,a)&#x3D;\vec{w}\cdot \vec{f}(s,a)<br>$$</p>
<p>其中$f_i$表示特征函数，比如在Pacman中可以是距离最近gohst的距离</p>
<p>对$V(s)$的重构使得我们的问题变成了学习参数$\vec{w}$，操作如下：</p>
<p>定义$diff&#x3D;[R(s,a,s’)+\gamma \space \underset{a’}{max}Q_k(s’,a’)]-Q(s,a)$, 则更新规则如下:<br>$$<br>w_i\leftarrow w_i+\alpha\cdot diff\cdot f_i(s,a)<br>$$<br>如此，我们就只需要在内存中维护$\vec{w}$, 在学习结束后按需计算$Q^{*}$即可。</p>
<blockquote>
<p>Q: AQ-L的idea和最终的算法是怎么联系到一起的？</p>
</blockquote>
<p>在上文中我们看到AQ-L的启发式动机是，在Q-L中学习了大量重复的内容。因此为了优化，我们希望的是对于相似的情况只学习一遍，其它情况由这一遍习得的结果推断得出。这个idea促使我们不再学习具体的state’s value，而是学习从相似states中抽象而来的特征的value. 而一旦抽象出特征，value就可立马表示成线性值函数，需要学习的内容进一步变成了值函数的参数。</p>
<h1 id="Exploration-and-Exploitation"><a href="#Exploration-and-Exploitation" class="headerlink" title="Exploration and Exploitation"></a>Exploration and Exploitation</h1><p><a name="exploration" alt="none"> </a></p>
<h2 id="epsilon-Greedy"><a href="#epsilon-Greedy" class="headerlink" title="$\epsilon$-Greedy"></a>$\epsilon$-Greedy</h2><p>想法非常简单，选定$\epsilon\in[0,1]$。然后以概率$\epsilon$采用random policy, 以概率$1-\epsilon$采用当前的estimated optimal policy. </p>
<p>在实际应用中$\epsilon$需要人为tune</p>
<h2 id="Exploration-Functions"><a href="#Exploration-Functions" class="headerlink" title="Exploration Functions"></a>Exploration Functions</h2><p>更新规则变为:<br>$$<br>Q(s,a)\leftarrow (1-\alpha)Q(s,a)+\alpha\cdot [R(s,a,s’)+\gamma \underset{a’}{max}f(s’,a’)]<br>$$<br>$f$有很多种定义，较为常见的是如下:<br>$$<br>f(s,a)&#x3D;Q(s,a)+\frac{k}{N(s,a)}<br>$$<br>其中$k$是一个提前确定的常数,$N(s,a)$表示该q-state被访问的次数</p>
<p><strong>关于Exploration和Exploitation的理解还不是很好，之后会做project3加深理解</strong></p>
</div><div class="tags"><a href="/tags/人工智能"><i class="fa fa-tag">人工智能</i></a></div><div class="post-nav"><a class="next" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://heiyan-2020.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img src="/img/avatar.jpg"/></a><p>态度是最重要的默认参数。</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15px;">人工智能</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E6%83%B3/" style="font-size: 15px;">生活随想</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/05/CS188-P5-Reinforcement-Learning/">CS188-P5 Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/21/CS188-P3-Game/">CS188-P3 Game</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/20/CS188-P2-CSP/">CS188-P2 CSP</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/18/CS188-P1-Searching/">CS188-P1 Searching</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/15/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%9F/">什么是计算思维？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/22/%E6%88%91%E7%9A%84%E6%96%B0%E5%A4%A9%E5%9C%B0/">我的新天地</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://icstorm.top/" title="永远の咣队" target="_blank">永远の咣队</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2022 <a href="/." rel="nofollow">黑岩.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>