<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>CS188-P3 Game | 黑岩</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CS188-P3 Game</h1><a id="logo" href="/.">黑岩</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CS188-P3 Game</h1><div class="post-meta">2022-08-21<span> | </span><span class="category"><a href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Minimax"><span class="toc-number">1.</span> <span class="toc-text">Minimax</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#alpha-beta-Pruning"><span class="toc-number">2.</span> <span class="toc-text">$\alpha-\beta$ Pruning</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Evaluation-Function"><span class="toc-number">3.</span> <span class="toc-text">Evaluation Function</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Expectimax"><span class="toc-number">4.</span> <span class="toc-text">Expectimax</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Utilities"><span class="toc-number">5.</span> <span class="toc-text">Utilities</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Project-2"><span class="toc-number">6.</span> <span class="toc-text">Project 2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Q1-Reflex-Agent"><span class="toc-number">6.1.</span> <span class="toc-text">Q1 Reflex Agent</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q2-Minimax"><span class="toc-number">6.2.</span> <span class="toc-text">Q2 Minimax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q3-Alpha%E2%80%94beta-pruning"><span class="toc-number">6.3.</span> <span class="toc-text">Q3 Alpha—beta pruning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q4-Expectimax"><span class="toc-number">6.4.</span> <span class="toc-text">Q4 Expectimax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q5-Better-Evaluation-Function"><span class="toc-number">6.5.</span> <span class="toc-text">Q5 Better Evaluation Function</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">7.</span> <span class="toc-text">总结</span></a></li></ol></div></div><div class="post-content"><p>零和博弈问题的求解策略 | CS188 Project 2 的实验分享</p>
<span id="more"></span>

<p>在传统的搜索问题中每一步的状态转移都是已知的(即一个状态可以转去哪些状态，代价分别是多少)，因此我们只需要搜到一条通往goal state的路径然后perform一下就好。但在博弈问题中，由于对手的存在我们并不能确定某个状态会转移到哪个其它状态。换句话说，在博弈问题中我们对状态转移没有完全控制权。因此，传统的搜索技术无法应用于这些问题。在本文中我们先来研究最简单的<strong>确定性零和博弈</strong>。确定性是指博弈过程中的行为都是确定的，不存在随机性；零和指我方的收益与对方的损失相当。</p>
<p>可以这样简单地想：在零和博弈中存在一个变量$X$，我方想要最大化$X$的取值而敌方则要最小化。双方的收益取决于同一个变量，这就是零和博弈最大的特征。</p>
<h1 id="Minimax"><a href="#Minimax" class="headerlink" title="Minimax"></a>Minimax</h1><p>我们首先做出如下假设：敌方的行为总是趋向于最小化$X$的值，即使得我们的状态最差；而我方总是趋向于最大化$X$的值。有了这样一个假设，双方在不同状态下如何迁移就都固定了，我们又重掌了对状态转移的完全控制权。</p>
<p>为了更精确地描述算法，我们形式化地定义博弈树如下：</p>
<ul>
<li><p>terminal utilities: 博弈结束状态的收益值，在一场博弈中属于先验知识。</p>
</li>
<li><p>state value: 一个状态的取值表示从这个状态出发agent所能获得的最大收益。</p>
</li>
</ul>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822002106395.png" alt="图1"></p>
<p>在上图所示的博弈树中，蓝色为我方控制的状态，红色为敌方控制的状态。根据我们之前做出的假设，terminal utilites将不断向上传播到根节点(具体实现为dfs)，传播的规则如下：</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822002218977.png" alt="图2"></p>
<h1 id="alpha-beta-Pruning"><a href="#alpha-beta-Pruning" class="headerlink" title="$\alpha-\beta$ Pruning"></a>$\alpha-\beta$ Pruning</h1><p>只要我们在最大最小值算法中的假设成立，该算法看似完美——即使敌人处处“为难”我们，我们也总能选取最终获得最大收益的行为。但该算法实际的时间复杂度为$O(b^m)$. ($b$指每个节点的最大分支数, $m$指树的高度). 我们必须采取一些手段缩减搜索空间，提高搜索效率。</p>
<p>Alpha-Beta剪枝的核心思想很简单：在遍历$n$的children时，一旦发现此时$n$的取值不可能被$n$的parent采用时，就停止对children的遍历。在有了剪枝后，算法复杂度缩减为$O(b^{\frac{m}{2}})$, 这使得我们原来可解的搜索深度扩大了一倍。</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822002908422.png" alt="图3"></p>
<p>下面来看一个简单的例子，在遍历中间结点$n$(图中第二层为2的结点)的children时，首先访问到2，因为$n$是最小值结点，因此$val(n)\leq 2$. 而$n$的父结点取值却$\geq 3$. $n$不会被父结点考虑了，因此就不用再继续遍历$n$的孩子了。</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822003114832.png" alt="图4"></p>
<p>伪代码如下：<img src="/2022/08/21/CS188-P3-Game/image-20220822003127472.png" alt="图5"></p>
<h1 id="Evaluation-Function"><a href="#Evaluation-Function" class="headerlink" title="Evaluation Function"></a>Evaluation Function</h1><p>当状态空间非常大时，搜索深度不能太大。但最大最小值算法要求我们必须搜到叶节点才能将terminal utilities传播上去。为了解决这个问题，我们的想法是为非叶节点提供评估函数估计其state value然后向上传播。</p>
<p>这种办法使得状态空间很大的问题也能应用最大最小值算法求解，但由于评估函数并不是真实值，所以最后根节点的收益不一定是最佳的。</p>
<p>评估函数的真实性会显著影响该算法找到最佳收益的能力。一般来说，评估函数是状态特征的线性组合。即:</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822003731387.png" alt="图6"></p>
<ul>
<li>$f_i(s)$是从状态$s$中提取出一些可被量化的特征。比如五子棋中当前连四子的个数</li>
<li>$w_i$是每个特征的权重，用于区分不同特征的重要程度。比如一些特征有利于敌方，那么$w$应当是负的。</li>
</ul>
<h1 id="Expectimax"><a href="#Expectimax" class="headerlink" title="Expectimax"></a>Expectimax</h1><p>上述三种trick构成了解决确定性零和博弈问题的框架，它们依赖于一个重要的假设即对方的行为是确定的且总是试图最小化我方的收益。但当对方的行为并不总是确定的而是有一定概率(已知)时，就需要对敌方的行为做出修正。办法是取敌方控制的状态的数学期望值。</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822110010897.png" alt="图7"></p>
<p>这个模型可以应对两种情况，一种是敌方的行为的结果是非确定的，每一步行为有概率$p$导致状态发生$s\to s’$的变化；另一种是敌方行为的结果确定但采取使得$s\to s’$的行为的概率为$p$</p>
<h1 id="Utilities"><a href="#Utilities" class="headerlink" title="Utilities"></a>Utilities</h1><p>上面讨论的4种算法已经能cover确定性零和博弈以及部分非确定性零和博弈了，它们能够在特定情况下找出使我方utilities最大的博弈策略。但我们还没有形式化地定义utility。事实上，utility是衔接计算本身与agent行为的关键，计算只是最大化utility，因此只有utility与rational高度相关时，我们的agent才能表现出ration. 所以合理地定义utility十分关键。</p>
<p>考虑如下定义：</p>
<ul>
<li>在B和A + $1中，agent偏向B</li>
<li>在C和B + $1中，agent偏向C</li>
<li>在A和C + $1中，agent偏向A</li>
</ul>
<p>现假设agent持有A，对手持有B和C，那么经过交换A将一无所有还搭上了$3…显然不够理性。可见，定义合理的utility是agent表现出理性的关键。下面我们来形式化地定义它：</p>
<p>定义agent总是需要在不同的<strong>price</strong>和<strong>lottery</strong>中做出抉择，price指一个固定的价值，lottery则根据固定的概率展现不同的price，以两个price为例：<br>$$<br>L&#x3D;[p,A;(1-p),B]<br>$$</p>
<ul>
<li>如果agent在price $A,B$ 中偏向于$A$，记为$A\succ B$</li>
<li>如果agent在price $A,B$ 中没有偏好，记为$A\sim B$</li>
</ul>
<p>要想agent表现出理性，那么它对不同物体的偏好应当满足如下公理：</p>
<ul>
<li><p>有序性：$(A ≻ B)∨(B ≻ A)∨(A ∼ B)$</p>
</li>
<li><p>传递性：$(A ≻ B)∧(B ≻ C) ⇒ (A ≻ C)$</p>
</li>
<li><p>连续性：$A ≻ B ≻ C ⇒ ∃p[p, A; (1− p), C] ∼ B$</p>
<p>当$B$在偏好序列的中间时，可以构造出一个有关$A,C$的lottery使得它和$B$的价值令agent没有偏好</p>
</li>
<li><p>替代性：$A ∼ B ⇒ [p, A; (1− p), C] ∼ [p, B; (1− p), C]$</p>
</li>
<li><p>单调性：$A ≻ B ⇒ (p ≥ q ⇔ [p, A; (1− p), B] ⪰ [q, A; (1−q), B]$</p>
</li>
</ul>
<p>上述公理只刻画了不同price、lottery之间的偏好<strong>顺序</strong>，然而要想被计算，我们还需要定义效用函数$U$将这些偏好顺序变为数值上的顺序。很直观地，我们只需要保证$U$满足如下规则即可：</p>
<ul>
<li>$U(A) ≥ U(B)⇔A ⪰ B$</li>
<li>$U([p_1, S_1; … ; p_n, S_n])&#x3D; ∑_ip_iU(S_i)$</li>
</ul>
<p>事实上，对于lottery，公理中并没有给出它和price或是其它lottery直接相比的规则，但赋予数值后它们都不可避免地可比了。而不同的效用函数会使这部分结果不尽相同，这也是符合理性的。因为人在概率事件上的表现也并不相同，有的人偏向保守，有的人偏向冒险。下面我们以一个例子说明：</p>
<p>假设有三个效用函数$U_1(x)&#x3D;x,U_2(x)&#x3D;\sqrt{x},U_3(x)&#x3D;x^2$</p>
<p>现有如下两个物体可被选择：$o_1&#x3D;500$, $o_2$是一张彩票，有50%的概率获得1000元。分别代入上述函数，$U_1$在两者之间没有偏好，$U_2$偏向保守，会选择$o_1$, 而$U_3$偏向冒险会选择$o_2$。这种结果从理性上也是说得通的</p>
<h1 id="Project-2"><a href="#Project-2" class="headerlink" title="Project 2"></a>Project 2</h1><h2 id="Q1-Reflex-Agent"><a href="#Q1-Reflex-Agent" class="headerlink" title="Q1 Reflex Agent"></a>Q1 Reflex Agent</h2><p>Reflex Agent就是不考虑行为的结果，单纯基于当前状态做出反应。有点类似于”贪心”思想。这道题目要求实现一个Reflex Pacman避开所有ghosts并吃到所有food. 实现很简单，根据当前状态扩展出Pacman可能的后继状态，利用评估函数为每个后继状态评分并选择最优的那一个。评估函数也很好写:</p>
<ol>
<li>先判断是否会遇到ghost，如果会则直接pass</li>
<li>再判断是否吃到了food，如果吃到则返回一个很大的收益</li>
<li>上述都不满足时，计算距离所有food的曼哈顿距离，返回最小距离的倒数</li>
</ol>
<p>上述算法其实相当简化：</p>
<ol>
<li>后继状态中ghost的位置有多种可能(四个方向)，为了简便我直接取它在上一个状态的位置+方向形成的新位置(事实上ghost可以变向)，因此依然有可能被吃掉</li>
<li>在有障碍物时，计算曼哈顿距离可能会导致pacman一直处于stop状态。如下图所示，当前Pacman所处位置其实是曼哈顿距离最短的，因此按照上述算法不走才是最优解。但计算真实距离又太慢了</li>
</ol>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822130712145.png" alt="图8"></p>
<p>好在这道题测试用例中并没有障碍物，做Q1也只是体验一下reflex agent，没必要深究。值得花时间当然是后面的plan agent啦</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220822130919429.png" alt="图9"></p>
<h2 id="Q2-Minimax"><a href="#Q2-Minimax" class="headerlink" title="Q2 Minimax"></a>Q2 Minimax</h2><p>很常规的写代码，没什么特殊的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getAction</span>(<span class="params">self, gameState</span>):</span><br><span class="line">    nextIsPacman = <span class="keyword">lambda</span> agentIdx: agentIdx == gameState.getNumAgents() - <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">miniMax</span>(<span class="params">currentState, agentIdx, depth</span>):</span><br><span class="line">        <span class="keyword">if</span> currentState.isWin() <span class="keyword">or</span> currentState.isLose() <span class="keyword">or</span> depth == self.depth + <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> (self.evaluationFunction(currentState), <span class="string">&#x27;Stop&#x27;</span>)</span><br><span class="line">        valueFunc = <span class="literal">None</span></span><br><span class="line">        nextAgentIdx = (agentIdx + <span class="number">1</span>) % currentState.getNumAgents()</span><br><span class="line">        <span class="keyword">if</span> nextIsPacman(agentIdx):</span><br><span class="line">            depth += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> agentIdx == <span class="number">0</span>:</span><br><span class="line">                valueFunc = <span class="built_in">max</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                valueFunc = <span class="built_in">min</span></span><br><span class="line">                legalActions = currentState.getLegalActions(agentIdx)</span><br><span class="line">                successorValues = [miniMax(currentState.generateSuccessor(agentIdx, action), nextAgentIdx, depth) <span class="keyword">for</span> action <span class="keyword">in</span> legalActions]</span><br><span class="line">                retValue = valueFunc(successorValues)</span><br><span class="line">                <span class="built_in">print</span>(retValue)</span><br><span class="line">                action = legalActions[successorValues.index(retValue)]</span><br><span class="line">                <span class="keyword">return</span> (retValue[<span class="number">0</span>], action)</span><br><span class="line">            </span><br><span class="line">     <span class="keyword">return</span> miniMax(gameState, <span class="number">0</span>, <span class="number">1</span>)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>值得注意的是，因为最终要返回最大value对应的action. 因此需要将action作为返回值一层层传递出去</p>
<h2 id="Q3-Alpha—beta-pruning"><a href="#Q3-Alpha—beta-pruning" class="headerlink" title="Q3 Alpha—beta pruning"></a>Q3 Alpha—beta pruning</h2><p>依然是很常规的写代码。稍微值得注意的是这道题目中max和min并不是交替的，而是有多个min层。但这并不影响alpha beta剪枝算法的核心，只需要根据agentIndex判断下一层是什么层即可</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getAction</span>(<span class="params">self, gameState</span>):</span><br><span class="line">        nextIsPacman = <span class="keyword">lambda</span> agentIdx: agentIdx == gameState.getNumAgents() - <span class="number">1</span></span><br><span class="line">        myMax = <span class="keyword">lambda</span> x, y: x <span class="keyword">if</span> x[<span class="number">0</span>] &gt; y[<span class="number">0</span>] <span class="keyword">else</span> y</span><br><span class="line">        myMin = <span class="keyword">lambda</span> x, y: x <span class="keyword">if</span> x[<span class="number">0</span>] &lt; y[<span class="number">0</span>] <span class="keyword">else</span> y</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">maxValue</span>(<span class="params">currentState, agentIdx, nextDepth, alpha, beta</span>):</span><br><span class="line">            maximal = (-<span class="number">9999</span>, <span class="literal">None</span>)</span><br><span class="line">            nextAgentIdx = (agentIdx + <span class="number">1</span>) % currentState.getNumAgents()</span><br><span class="line">            legalActions = currentState.getLegalActions(agentIdx)</span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> legalActions:</span><br><span class="line">                successorState = currentState.generateSuccessor(agentIdx, action)</span><br><span class="line">                maximal = myMax(maximal, (genericValue(successorState, nextAgentIdx, nextDepth, alpha, beta)[<span class="number">0</span>], action))</span><br><span class="line">                <span class="keyword">if</span> maximal[<span class="number">0</span>] &gt; beta:</span><br><span class="line">                    <span class="keyword">return</span> maximal</span><br><span class="line">                alpha = <span class="built_in">max</span>(alpha, maximal[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">return</span> maximal</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">minValue</span>(<span class="params">currentState, agentIdx, nextDepth, alpha, beta</span>):</span><br><span class="line">            minimal = (<span class="number">9999</span>, <span class="literal">None</span>)</span><br><span class="line">            nextAgentIdx = (agentIdx + <span class="number">1</span>) % currentState.getNumAgents()</span><br><span class="line">            legalActions = currentState.getLegalActions(agentIdx)</span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> legalActions:</span><br><span class="line">                successorState = currentState.generateSuccessor(agentIdx, action)</span><br><span class="line">                minimal = myMin(minimal, (genericValue(successorState, nextAgentIdx, nextDepth, alpha, beta)[<span class="number">0</span>], action))</span><br><span class="line">                <span class="keyword">if</span> minimal[<span class="number">0</span>] &lt; alpha:</span><br><span class="line">                    <span class="keyword">return</span> minimal</span><br><span class="line">                beta = <span class="built_in">min</span>(beta, minimal[<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">return</span> minimal</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">genericValue</span>(<span class="params">currentState, agentIdx, depth, alpha, beta</span>):</span><br><span class="line">            <span class="keyword">if</span> currentState.isWin() <span class="keyword">or</span> currentState.isLose() <span class="keyword">or</span> depth == self.depth + <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> (self.evaluationFunction(currentState), <span class="string">&#x27;Stop&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> nextIsPacman(agentIdx):</span><br><span class="line">                depth += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> agentIdx == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> maxValue(currentState, agentIdx, depth, alpha, beta)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> minValue(currentState, agentIdx, depth, alpha, beta)</span><br><span class="line">        <span class="keyword">return</span> genericValue(gameState, <span class="number">0</span>, <span class="number">1</span>, -<span class="number">9999</span>, <span class="number">9999</span>)[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Q4-Expectimax"><a href="#Q4-Expectimax" class="headerlink" title="Q4 Expectimax"></a>Q4 Expectimax</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">getAction</span>(<span class="params">self, gameState</span>):</span><br><span class="line">        nextIsPacman = <span class="keyword">lambda</span> agentIdx: agentIdx == gameState.getNumAgents() - <span class="number">1</span></span><br><span class="line">        myMax = <span class="keyword">lambda</span> x, y: x <span class="keyword">if</span> x[<span class="number">0</span>] &gt; y[<span class="number">0</span>] <span class="keyword">else</span> y</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">maxValue</span>(<span class="params">currentState, agentIdx, nextDepth</span>):</span><br><span class="line">            maximal = (-<span class="number">9999</span>, <span class="literal">None</span>)</span><br><span class="line">            nextAgentIdx = (agentIdx + <span class="number">1</span>) % currentState.getNumAgents()</span><br><span class="line">            legalActions = currentState.getLegalActions(agentIdx)</span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> legalActions:</span><br><span class="line">                successorState = currentState.generateSuccessor(agentIdx, action)</span><br><span class="line">                maximal = myMax(maximal, (genericValue(successorState, nextAgentIdx, nextDepth)[<span class="number">0</span>], action))</span><br><span class="line">            <span class="keyword">return</span> maximal</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">chanceValue</span>(<span class="params">currentState, agentIdx, nextDepth</span>):</span><br><span class="line">            expection = <span class="number">0</span></span><br><span class="line">            nextAgentIdx = (agentIdx + <span class="number">1</span>) % currentState.getNumAgents()</span><br><span class="line">            legalActions = currentState.getLegalActions(agentIdx)</span><br><span class="line">            <span class="keyword">for</span> action <span class="keyword">in</span> legalActions:</span><br><span class="line">                successorState = currentState.generateSuccessor(agentIdx, action)</span><br><span class="line">                expection += genericValue(successorState, nextAgentIdx, nextDepth)[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">return</span> (expection / <span class="built_in">len</span>(legalActions), <span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">genericValue</span>(<span class="params">currentState, agentIdx, depth</span>):</span><br><span class="line">            <span class="keyword">if</span> currentState.isWin() <span class="keyword">or</span> currentState.isLose() <span class="keyword">or</span> depth == self.depth + <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> (self.evaluationFunction(currentState), <span class="string">&#x27;Stop&#x27;</span>)</span><br><span class="line">            <span class="keyword">if</span> nextIsPacman(agentIdx):</span><br><span class="line">                depth += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> agentIdx == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> maxValue(currentState, agentIdx, depth)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> chanceValue(currentState, agentIdx, depth)</span><br><span class="line">        value = genericValue(gameState, <span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> value[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<h2 id="Q5-Better-Evaluation-Function"><a href="#Q5-Better-Evaluation-Function" class="headerlink" title="Q5 Better Evaluation Function"></a>Q5 Better Evaluation Function</h2><p>根据笔记中对评估函数的描述，我们只需要将一些重要的特征提取出来然后赋予合适的权重即可。</p>
<p>这道题目中我取出的特征是距离ghost的距离和距离食物距离的倒数，然后通过“调参”确定了还可以的权重。</p>
<p>整个过程中有点奇怪的地方是，在food只剩1-2个的时候，pacman会一直原地上下晃动而不是去吃food。原因在于我的参数使得吃那1-2个food的收益小于远离ghost的收益，所以我就特判了一下在food数量较少时提高food的权重。</p>
<p>这部分代码因人而异就不放出来了。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这次的project比较简单，需要自己思考的内容不多，主要还是实现几个博弈树算法。但让我觉得有点惊喜的是，在Q5中切实体会了一把“调参”的感觉，即使是这样一个简单的问题，依然可以不仔细思考参数的作用而凭直觉调整来看效果。</p>
<p><img src="/2022/08/21/CS188-P3-Game/image-20220823110744667.png" alt="图10"></p>
</div><div class="tags"><a href="/tags/人工智能"><i class="fa fa-tag">人工智能</i></a></div><div class="post-nav"><a class="pre" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a><a class="next" href="/2022/08/20/CS188-P2-CSP/">CS188-P2 CSP</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://heiyan-2020.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img src="/img/avatar.jpg"/></a><p>态度是最重要的默认参数。</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E6%80%BB%E7%BB%93/">总结</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15px;">人工智能</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E6%83%B3/" style="font-size: 15px;">生活随想</a> <a href="/tags/%E8%87%AA%E6%88%91%E6%8E%A2%E7%B4%A2/" style="font-size: 15px;">自我探索</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2023/04/01/%E5%A4%A7%E4%B8%89%E4%B8%8B%E6%80%BB%E7%BB%93/">大三下流水账</a></li><li class="post-list-item"><a class="post-list-link" href="/2023/03/26/%E5%85%AD%E4%B8%AA%E6%9C%88%E4%B8%8D%E8%A7%81%EF%BC%8C%E6%88%91%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/">六个月不见，我改变了什么？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/09/05/CS188-P5-Reinforcement-Learning/">CS188-P5 Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/21/CS188-P3-Game/">CS188-P3 Game</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/20/CS188-P2-CSP/">CS188-P2 CSP</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/18/CS188-P1-Searching/">CS188-P1 Searching</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/15/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%9F/">什么是计算思维？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/22/%E6%88%91%E7%9A%84%E6%96%B0%E5%A4%A9%E5%9C%B0/">我的新天地</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://icstorm.top/" title="永远の咣队" target="_blank">永远の咣队</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">黑岩.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>