<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description" content=""><title>CS188-P4 Markov decision process | 黑岩</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=1.0.0"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/normalize/latest/normalize.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/pure-min.min.css"><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/pure/latest/grids-responsive-min.min.css"><link rel="stylesheet" href="//lib.baomitu.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//lib.baomitu.com/jquery/latest/jquery.min.js"></script><link rel="icon" mask="" sizes="any" href="/favicon.ico"><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"><script type="text/javascript" src="//lib.baomitu.com/clipboard.js/latest/clipboard.min.js"></script><script type="text/javascript" src="//lib.baomitu.com/toastr.js/latest/toastr.min.js"></script><link rel="stylesheet" href="//lib.baomitu.com/toastr.js/latest/toastr.min.css"><meta name="generator" content="Hexo 6.2.0"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">CS188-P4 Markov decision process</h1><a id="logo" href="/.">黑岩</a><p class="description"></p></div><div id="nav-menu"><a class="current" href="/."><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div class="pure-g" id="layout"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">CS188-P4 Markov decision process</h1><div class="post-meta">2022-08-23<span> | </span><span class="category"><a href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></span></div><div class="clear"><div class="toc-article" id="toc"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Dynamic-Programming"><span class="toc-number">1.</span> <span class="toc-text">Dynamic Programming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Dynamic-decision-problem"><span class="toc-number">1.1.</span> <span class="toc-text">Dynamic decision problem</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Bellman-Equation"><span class="toc-number">1.2.</span> <span class="toc-text">Bellman Equation</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Marcov-decision-process"><span class="toc-number">2.</span> <span class="toc-text">Marcov decision process</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Solve-the-Bellman-Equation"><span class="toc-number">3.</span> <span class="toc-text">Solve the Bellman Equation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Value-Iteration"><span class="toc-number">3.1.</span> <span class="toc-text">Value Iteration</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Extraction"><span class="toc-number">3.2.</span> <span class="toc-text">Policy Extraction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Policy-Iteration"><span class="toc-number">3.3.</span> <span class="toc-text">Policy Iteration</span></a></li></ol></li></ol></div></div><div class="post-content"><p>在<a href="https://heiyan-2020.github.io/2022/08/18/CS188-P1-Searching/">P1</a>和<a href="https://heiyan-2020.github.io/2022/08/21/CS188-P3-Game/">P3</a>中我们分别讨论了传统的搜索问题以及确定性零和博弈问题，它们都有一个共同的特征: 在状态$s$采取行为$a$，发生的状态转移是确定的。然而，实际生活中有非常多的问题充满不确定性（像扑克、麻将等），因此我们需要对这种不确定性进行建模————<strong>马尔科夫决策过程</strong>(MDP)。</p>
<h1 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h1><p>不同于算法课中的动态规划算法，这里的DP指的是一种数学优化方法。</p>
<h2 id="Dynamic-decision-problem"><a href="#Dynamic-decision-problem" class="headerlink" title="Dynamic decision problem"></a>Dynamic decision problem</h2><p>DP要解决的问题统称为dynamic decision problem, 它们具有如下形式:</p>
<ul>
<li>在$t$时刻的状态称为$x_t$。特别地，初始状态记为$x_0$</li>
<li>在任意时刻可采取的行动取决于当前的状态，记为$a_t\in\Gamma(x_t)$</li>
<li>在状态$x$采取行动$a$将转移到的状态由$T(x,a)$确定，收益由$F(x,a)$确定. (先不考虑随机性)</li>
<li>引入discount factor $\gamma$表示收益随着时间指数级递减</li>
<li>$V(x)$表示从状态$x$出发能获得的最大收益</li>
</ul>
<p>则有:<br>$$<br>V(x_0)&#x3D;max_{a_t}\Sigma_{t&#x3D;0}^{\infin}\gamma ^t F(x_t,a_t)<br>$$<br>直观来讲就是遍历所有可能采取的行动的组合，最大值则为初始状态能获得的最大收益。</p>
<h2 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h2><p>上式显然是难以直接计算的。但根据定义，我们可以得出如下结论：</p>
<blockquote>
<p>Principle of Optimality: 一个最优策略具有这样的性质————无论初始状态$x_0$和初始决策$a_)$是什么，之后的决策对于$T(x_0,a_0)$都构成最优策略。</p>
</blockquote>
<p>这个结论其实就是DP算法中的最优子结构。有了这样一个结论，上式就可改写为递推式：<br>$$<br>V(x_0)&#x3D;\underset{a_0}{max}{F(x_0,a_0)+\gamma V(T(x_0,a_0))}<br>$$<br>这就是<strong>贝尔曼方程</strong>。此时，求解(1)式就变成了求解(2)中的函数方程。</p>
<p>如果引入了不确定性，即之前的$T(x,a)$有多个解，我们只需要转而求数学期望即可。</p>
<h1 id="Marcov-decision-process"><a href="#Marcov-decision-process" class="headerlink" title="Marcov decision process"></a>Marcov decision process</h1><ul>
<li>$S$是所有states的集合</li>
<li>$A$是所有actions的集合</li>
<li>start state</li>
<li>one or more terminal states</li>
<li>discount factor $\gamma$</li>
<li>转移函数$T(s,a,s’)$, 值为在状态$s$采取action $a$，状态转移到$s’$的概率</li>
<li>回报函数$R(s,a,s’)$, 值为在状态$s$采取action $a$，状态转移到$s’$的回报。通常来说每一步都有一个小回报，转移到terminal state则是大回报，用于刻画实际决策过程。</li>
</ul>
<p><img src="/2022/08/23/CS188-P4-Markov-decision-process/image-20220829232827120.png" alt="图1"></p>
<p>上图是一个具体的例子，汽车有三种状态，在每种状态下采取行动会以不同概率转移到其它状态。</p>
<p>通常，对于一个MDP问题，我们希望求解的是<strong>在到达terminal state之前，如何决策才能使回报最大</strong>。有时，我们还希望限制决策的时间，即要在有限时间内回报最大。以上图为例，如果不限时间，永远在Cool状态选择slow action，将永远不会到达terminal state，回报也一直在上升。</p>
<p><img src="/2022/08/23/CS188-P4-Markov-decision-process/image-20220829234118173.png" alt="图2"></p>
<p>为了建模不确定性，在状态树中我们引入$q-state$节点(上图中的绿点)与正常的$state$节点区分，一条$state$节点到$q-state$节点的边代表一个action，$q-state$到$state$的边则代表采取该行动后转移到目标状态的概率。</p>
<p>显而易见，MDP符合Principle of Optimality. 它对应的贝尔曼方程如下：<br>$$<br>V^{<em>}(s)&#x3D;\underset{a}{max}\underset{s’}{\sum}\space T(s,a,s’)[R(s,a,s’)+\gamma V^{</em>}(s’)]<br>$$<br>甚至可以更清晰的解释该方程，我们引入$Q(s,a)$表示从状态$s$采取行动$a$到达的$q&#x3D;state$的最优收益。其计算式如下：<br>$$<br>Q^{<em>}(s,a)&#x3D;\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V^{</em>}(s’)]<br>$$<br>进一步有:<br>$$<br>V^{<em>}(s)&#x3D;\underset{a}{max}\space Q^{</em>}(s,a)<br>$$<br>事实上，如果能找到上述方程的一个解$V$，则必有$\forall s\in S.V(s)&#x3D;V^{*}(s)$. <a target="_blank" rel="noopener" href="https://halshs.archives-ouvertes.fr/halshs-01159177/document">证明</a></p>
<h1 id="Solve-the-Bellman-Equation"><a href="#Solve-the-Bellman-Equation" class="headerlink" title="Solve the Bellman Equation"></a>Solve the Bellman Equation</h1><h2 id="Value-Iteration"><a href="#Value-Iteration" class="headerlink" title="Value Iteration"></a>Value Iteration</h2><p>顾名思义，值迭代算法通过不断迭代$V$直到收敛来求解方程。</p>
<p>记第$i$次迭代时的Value Function为$V_{i}$, 同时它也代表在时间限制为$i$时的最大收益，算法具体流程如下：</p>
<ul>
<li><p>$\forall s\in S.V_0(s)&#x3D;0$</p>
</li>
<li><p>重复下面的更新，直到$\forall s,V_{k+1}(s)&#x3D;V_k(s)$</p>
</li>
</ul>
<p>$$<br>\forall s\in S,V_{k+1}(s)\leftarrow \underset{a}{max}\underset{s’}{\sum}\space T(s,a,s’)[R(s,a,s’)+\gamma V_k(s’)]<br>$$</p>
<p>时间限制为$k+1$时，从$s$转移到后继状态必然占用一个时间戳，因此其子问题变为时间限制为$k$。</p>
<p>这种迭代实质上是一种动态规划算法，以时间限制为计算顺序。</p>
<p>为什么收敛后一定与$V^{*}(s)$相等?</p>
<ul>
<li>从时间限制的角度来看，每一次迭代都求得时间限制为$k$时的真实最优值，当这个$k&#x3D;k_0$时收敛，说明$\forall k\ge k_0.V_k(s)&#x3D;V_{k_0}(s)$. 也就是说时间限制再怎么放大甚至到无穷，最优值都是$V_{k_0}(s)$</li>
</ul>
<h2 id="Policy-Extraction"><a href="#Policy-Extraction" class="headerlink" title="Policy Extraction"></a>Policy Extraction</h2><p>仅仅得出MDP的最优值函数是不够的，我们真正需要的是其对应的Policy. 这个也很简单，当值函数收敛时，每一个状态与其最大的$q-state$之间的action即为Policy在这一步的取值。<br>$$<br>∀s ∈ S, π∗(s) &#x3D; \underset{a}{argmax}<br>\space Q^∗(s,a) &#x3D; \underset{a}{argmax}\sum_{s’}<br>T(s,a,s′)[R(s,a,s′)+γV ∗(s′)]<br>$$<br>为了减少重复计算，在Value Iteration时额外储存所有q-state节点的q-value，这样上式就只需要一个argmax操作即可</p>
<h2 id="Policy-Iteration"><a href="#Policy-Iteration" class="headerlink" title="Policy Iteration"></a>Policy Iteration</h2><p>在Value Iteration中，每次迭代需要遍历$|S|$个状态，每个状态至多有$|A|$个actions，每个action至多会涉及$|S|$个后继状态。因此总的时间复杂度是$O(|S|^2|A|)$. 这还只是一次迭代。</p>
<p>实际上，我们真正需要的是optimal policy而非精确的optimal value，而policy收敛要比value快得多。（一个值函数可能并非$V^*$，但其在收敛到$V^*$的过程中可能policy不会改变了）。Policy Iteration算法就是只让Policy收敛即可，算法流程如下：</p>
<ol>
<li><p>定义一个初始的policy，可以是随机的，也可以用一些启发式方法使其接近optimal policy加快收敛</p>
</li>
<li><p>重复以下操作直到收敛，即$\forall s\in S.\pi_{i+1}(s)&#x3D;\pi(s)$：</p>
<ul>
<li><p>对当前的policy进行评估。定义$V^\pi(s)$为在采取policy $\pi$的情况下从$s$出发获得的实际收益。policy evaluation就是计算所有$s$的$V^\pi$, 即取遍$s\in S$，求解下列方程组成的系统。<br>$$<br>V^\pi(s)&#x3D;\sum_{s’}T(s,\pi(s),s’)[R(s,\pi(s),s’)+\gamma V^\pi(s’)]<br>$$</p>
</li>
<li><p>当对目前policy有评估后，就可以利用这个评估优化policy。<br>$$<br>\pi_{i+1}(s)&#x3D;\underset{a}{argmax}\sum_{s’}T(s,a,s’)[R(s,a,s’)+\gamma V^{\pi_i}(s’)]<br>$$</p>
</li>
</ul>
</li>
</ol>
<p>算法正确性：第一次迭代会使得termial state上层节点采取正确的policy，后续迭代则会将这种正确性自底向上传递。因此，理论上迭代到最后一定会达到optimal policy，而收敛使得迭代结果不再改变，因此是为optimal policy。</p>
</div><div class="tags"><a href="/tags/人工智能"><i class="fa fa-tag">人工智能</i></a></div><div class="post-nav"><a class="pre" href="/2022/09/05/%E5%85%AD%E4%B8%AA%E6%9C%88%E4%B8%8D%E8%A7%81%EF%BC%8C%E6%88%91%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/">CS188-P5 Reinforcement Learning</a><a class="next" href="/2022/08/21/CS188-P3-Game/">CS188-P3 Game</a></div></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form class="search-form" action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://heiyan-2020.github.io"/></form></div><div class="widget"><div class="author-info"><a class="info-avatar" href="/about/" title="About"><img src="/img/avatar.jpg"/></a><p>态度是最重要的默认参数。</p><a class="info-icon" href="https://twitter.com/username" title="Twitter" target="_blank" style="margin-inline:5px"> <i class="fa fa-twitter-square" style="margin-inline:5px"></i></a><a class="info-icon" href="mailto:admin@domain.com" title="Email" target="_blank" style="margin-inline:5px"> <i class="fa fa-envelope-square" style="margin-inline:5px"></i></a><a class="info-icon" href="https://github.com/username" title="Github" target="_blank" style="margin-inline:5px"> <i class="fa fa-github-square" style="margin-inline:5px"></i></a><a class="info-icon" href="/atom.xml" title="RSS" target="_blank" style="margin-inline:5px"> <i class="fa fa-rss-square" style="margin-inline:5px"></i></a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div><ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/%E7%AC%94%E8%AE%B0/">笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/">读书笔记</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" style="font-size: 15px;">人工智能</a> <a href="/tags/%E7%AE%97%E6%B3%95/" style="font-size: 15px;">算法</a> <a href="/tags/%E7%94%9F%E6%B4%BB%E9%9A%8F%E6%83%B3/" style="font-size: 15px;">生活随想</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/2022/09/05/CS188-P5-Reinforcement-Learning/">CS188-P5 Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/09/05/%E5%85%AD%E4%B8%AA%E6%9C%88%E4%B8%8D%E8%A7%81%EF%BC%8C%E6%88%91%E6%94%B9%E5%8F%98%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F/">CS188-P5 Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/23/CS188-P4-Markov-decision-process/">CS188-P4 Markov decision process</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/21/CS188-P3-Game/">CS188-P3 Game</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/20/CS188-P2-CSP/">CS188-P2 CSP</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/18/CS188-P1-Searching/">CS188-P1 Searching</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/08/15/%E4%BB%80%E4%B9%88%E6%98%AF%E8%AE%A1%E7%AE%97%E6%80%9D%E7%BB%B4%EF%BC%9F/">什么是计算思维？</a></li><li class="post-list-item"><a class="post-list-link" href="/2022/07/22/%E6%88%91%E7%9A%84%E6%96%B0%E5%A4%A9%E5%9C%B0/">我的新天地</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://icstorm.top/" title="永远の咣队" target="_blank">永远の咣队</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2023 <a href="/." rel="nofollow">黑岩.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a class="show" id="rocket" href="#top"></a><script type="text/javascript" src="/js/totop.js?v=1.0.0" async></script><script type="text/javascript" src="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=1.0.0" async></script><link rel="stylesheet" type="text/css" href="//lib.baomitu.com/fancybox/latest/jquery.fancybox.min.css"><script type="text/javascript" src="/js/copycode.js?v=1.0.0" successtext="Copy Successed!"></script><link rel="stylesheet" type="text/css" href="/css/copycode.css?v=1.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=1.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=1.0.0"></script></div></body></html>